tool:
  name: policy-foo
  description: Multi-agent system for policy search, analysis and answering user queries
  status: proof_of_concept

frontend:
  interface:
    layout:
      - navigation_bar: top
      - conversation_panel:
          width: 75%
          components:
            - rate_limit_display:
                position: top-right
                style: floating
                fields:
                  - hourly_remaining
                  - daily_remaining
            - chat_history:
                retention: session
                max_messages: 10
                display:
                  - user_messages:
                      actions: [edit, resubmit]
                      style: bg-gray-50
                  - ai_responses:
                      style: bg-white
                      sections:
                        - answer
                        - citations
                        - follow_up
            - input_area:
                type: expandable_textarea
                placeholder: "Ask about CAF policies..."
                auto_resize: true
            - submit_button:
                type: streaming_disabled
                icon: paper-plane
                label: Send

backend:
  agents:
    policyFinder:
      role: identify relevant policies for the down-stream agents
      system_prompt: /prompts/policyFinder.md
      input:
        - systemPrompt: policyFinder.md + DOAD-list-table.md
        - userPrompt: user query
        - conversation_history: last_5_exchanges
      output:
        format: comma-separated-string
        validation: Simple parsing of the policy numbers
        max_policies: 5
      logging:
        prefix: "[PolicyFinder]"
        debug_mode:
          - llm_responses: conditional
          - message_history: conditional
        info_mode:
          - found_policies: always
      main_LLM_API_parameters:
        model: POLICY_FINDER_MODEL
        temperature: 0.1
        stream: true
      backup_LLM_API_parameters:
        model: BACKUP_POLICY_FINDER_MODEL
        num_ctx: BACKUP_POLICY_FINDER_NUM_CTX
        batch_size: BACKUP_POLICY_FINDER_BATCH_SIZE
        temperature: 0.1
        stream: true
        execution:
          type: sequential
          timeout: 60s

    policyReader:
      role: extract relevant content from the policy
      system_prompt: /prompts/policyReader.md
      input:
        - systemPrompt: policyReader.md + {{policy_number}}.md
        - userPrompt: user query
        - conversation_history: last_5_exchanges
      output:
        format: xml
        sections:
          - policy_number
          - section
          - content
        validation: none
      logging:
        prefix: "[PolicyReader]"
        debug_mode:
          - llm_responses: conditional
          - message_history: conditional
          - policy_responses: conditional
        info_mode:
          - found_policies: always
      knowledge_base:
        - /policies/doad/{{policy_number}}.md
      main_LLM_API_parameters:
        model: POLICY_READER_MODEL
        temperature: 0.1
        stream: true
        execution:
          type: parallel
          stagger: 250ms
          timeout: 35s
      backup_LLM_API_parameters:
        model: BACKUP_POLICY_READER_MODEL
        num_ctx: BACKUP_POLICY_READER_NUM_CTX
        batch_size: BACKUP_POLICY_READER_BATCH_SIZE
        temperature: 0.1
        stream: true
        execution:
          type: sequential
          timeout: 60s 

    chatAgent:
      role: user-interaction
      system_prompt: /prompts/chatAgent.md
      input:
        - systemPrompt: chatAgent.md + {{policy_extracts}}
        - userPrompt: user query
        - conversation_history: last_5_exchanges
      output:
        format: xml
        sections:
          - answer
          - citations
          - follow_up
      logging:
        prefix: "[ChatAgent]"
        debug_mode:
          - llm_responses: conditional
          - message_history: conditional
          - request_details: conditional
      main_LLM_API_parameters:
        model: CHAT_AGENT_MODEL
        temperature: 0.1
        stream: true
      backup_LLM_API_parameters:
        model: BACKUP_CHAT_AGENT_MODEL
        num_ctx: BACKUP_CHAT_AGENT_NUM_CTX
        batch_size: BACKUP_CHAT_AGENT_BATCH_SIZE
        temperature: 0.1
        stream: true

  endpoints:
    - /llm/policyfoo/generate:
        methods: [POST]
        parameters:
          - content: string
          - conversation_history: array
          - temperature: float = 0.1
          - stream: bool = true
        response:
          format: text/plain
          fields:
            - content: string
            - remaining_requests:
                hourly_remaining: int
                daily_remaining: int

workflow_manager:
  orchestration:
    - initialize:
        action: receive_query
        handler: fastapi_endpoint
        endpoint: /llm/policyfoo/generate
        method: POST
        validation:
          - rate_limiting
    
    - find_policies:
        action: policy_identification
        handler: policyFinder
        error_handling:
          retry_count: 1
          fallback: empty_list
    
    - read_policies:
        action: parallel_extraction
        handler: policyReader
        timeout: 35.0
        stagger_delay: 0.25
    
    - generate_response:
        action: create_response
        handler: chatAgent
        format: xml
        sections:
          - answer
          - citations
          - follow_up
    
    - finish:
        action: update_session
        handler: session_manager
        operations:
          - update_history
          - update_rate_limits

state_management:
  session:
    storage: in_memory
    conversation_history:
      max_length: 10
      structure:
        - role: string
        - content: string
      pruning: fifo

monitoring:
  logging:
    privacy:
      never_logged:
        - user_messages
        - personal_info
        - session_data
    debug_mode:
      enabled_by: logger.isEnabledFor(10)
      includes:
        - llm_responses
        - message_history
        - request_details
    info_mode:
      always_log:
        - found_policies
        - api_call_status
  metrics:
    - agent_performance:
        - latency
        - success_rate
        - token_usage
    - user_interactions:
        - query_patterns
        - policy_coverage
    - system_health:
        - agent_availability
        - response_times
    - error_tracking:
        - agent_failures
        - workflow_breaks
